{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Playwright Scraping Notebook\n",
        "\n",
        "This notebook demonstrates how to use Playwright to scrape AI news from Wired.\n",
        "It directly interacts with the webpage to extract titles and URLs, then displays the results in a pandas DataFrame."
      ],
      "metadata": {
        "id": "xbIZQvpXELya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup and Installation ---\n",
        "# Install required libraries\n",
        "!pip install playwright pandas -q\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "id": "3nYpRuwhESZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "IpS43EUsEXqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply nest_asyncio to allow asynchronous operations in Jupyter\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FaFMq4KhEZSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Web Scraping Function ---\n",
        "async def scrape_wired_news(url):\n",
        "    async with async_playwright() as p:\n",
        "        # Launch the browser\n",
        "        browser = await p.chromium.launch()\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Navigate to the Wired AI news page\n",
        "        await page.goto(url)\n",
        "\n",
        "        # Select all news items on the page\n",
        "        news_items = await page.query_selector_all('.summary-item')\n",
        "\n",
        "        results = []\n",
        "        for item in news_items:\n",
        "            # Extract title and URL for each news item\n",
        "            title_element = await item.query_selector('.summary-item__hed')\n",
        "            link_element = await item.query_selector('.summary-item__hed-link')\n",
        "\n",
        "            if title_element and link_element:\n",
        "                title = await title_element.inner_text()\n",
        "                url = await link_element.get_attribute('href')\n",
        "                results.append({'title': title, 'url': f\"https://www.wired.com{url}\"})\n",
        "\n",
        "        await browser.close()\n",
        "        return results"
      ],
      "metadata": {
        "id": "n0TD6q7FEh2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape the news\n",
        "news = await scrape_wired_news(\"https://www.wired.com/tag/artificial-intelligence/?page=1\")\n",
        "\n",
        "# Convert results to a pandas DataFrame\n",
        "df = pd.DataFrame(news)"
      ],
      "metadata": {
        "id": "nZy88i53Ekp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(df.to_html(render_links=True, escape=False)))"
      ],
      "metadata": {
        "id": "ktAMRlaXFBrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}