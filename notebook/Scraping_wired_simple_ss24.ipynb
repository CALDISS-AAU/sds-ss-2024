{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wWHiJAjVKhTd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests beautifulsoup4 pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "b_HW4j5OKzBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_wired_article(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract title\n",
        "        title = soup.find('h1', class_='content-header__row content-header__hed')\n",
        "        title = title.text.strip() if title else 'Title not found'\n",
        "\n",
        "        # Extract main content\n",
        "        content = soup.find('div', class_='body__inner-container')\n",
        "        if content:\n",
        "            paragraphs = content.find_all('p')\n",
        "            text = ' '.join([p.text for p in paragraphs])\n",
        "        else:\n",
        "            text = 'Content not found'\n",
        "\n",
        "        # Extract author\n",
        "        author = soup.find('a', class_='byline__name')\n",
        "        author = author.text.strip() if author else 'Author not found'\n",
        "\n",
        "        # Extract date\n",
        "        date = soup.find('time', class_='content-header__publish-date')\n",
        "        date = date['datetime'] if date else 'Date not found'\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': title,\n",
        "            'text': text,\n",
        "            'author': author,\n",
        "            'date': date\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "by-cT0W0K20i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_multiple_articles(urls):\n",
        "    results = []\n",
        "    for url in tqdm(urls, desc=\"Scraping articles\"):\n",
        "        article_data = scrape_wired_article(url)\n",
        "        if article_data:\n",
        "            results.append(article_data)\n",
        "        time.sleep(random.uniform(1, 3))  # Random delay between requests\n",
        "    return results"
      ],
      "metadata": {
        "id": "Lg5Gx2SIK87j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your URLs (replace this with your actual method of loading URLs)\n",
        "urls = [\n",
        "    \"https://www.wired.com/story/applebot-extended-apple-ai-scraping/\",\n",
        "    \"https://www.wired.com/story/undress-app-ai-harm-google-apple-login/\",\n",
        "    # Add more URLs here\n",
        "]"
      ],
      "metadata": {
        "id": "BGtvnGtyLHYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape the articles\n",
        "articles = scrape_multiple_articles(urls)\n"
      ],
      "metadata": {
        "id": "iVlRvL2SLKkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(articles)\n"
      ],
      "metadata": {
        "id": "MwoM0hdfMiT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "xyNzdWwUMof4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "df.to_csv('wired_articles.csv', index=False)"
      ],
      "metadata": {
        "id": "Rg77PN8RMnSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}