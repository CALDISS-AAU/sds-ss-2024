{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJUBPlcX789"
      },
      "source": [
        "# Enhancing Topic Modeling with Open Source Large Language Models (LLMs) ü¶ô\n",
        "*Integrate BERTopic and LLMs for Richer Topic Insights*\n",
        "\n",
        "[Inspired by MAARTEN GROOTENDORST](https://maartengrootendorst.substack.com/p/topic-modeling-with-llama-2)\n",
        "<br>\n",
        "\n",
        "In this guide, we'll delve into leveraging open-source Large Language Models (LLMs) like Mistral, Llama, Gemma (and also Claude or GPT) and others for efficient Topic Modeling. Our focus will be on avoiding the exhaustive approach of processing each document with an LLM. We'll employ BERTopic, a flexible topic modeling framework that can utilize any LLM to refine topic delineations.\n",
        "\n",
        "BERTopic simplifies the process into five clear steps: embedding documents, dimensionality reduction of embeddings, clustering of embeddings, document tokenization by cluster, and extraction of the most representative words for each topic.\n",
        "<br>\n",
        "<div>\n",
        "<img src=\"https://github.com/MaartenGr/BERTopic/assets/25746895/e9b0d8cf-2e19-4bf1-beb4-4ff2d9fa5e2d\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "With the advent of advanced LLMs like **Llama, Mistral or Gemma**, our capacity for topic modeling has greatly expanded beyond simple word lists. Direct analysis of all documents by Llama 2 is computationally impractical. Although vector databases offer a solution for search, determining the precise topics of interest remains a challenge.\n",
        "\n",
        "We propose a novel approach: utilizing BERTopic to generate clusters and topics, then employing Mixtral to refine and enhance these into more precise topic representations.\n",
        "\n",
        "This method merges the strengths of both worlds: BERTopic's efficient topic generation and Mixtral's refined topic representation.\n",
        "<br>\n",
        "<div>\n",
        "<img src=\"https://github.com/MaartenGr/BERTopic/assets/25746895/7c7374a1-5b41-4e93-aafd-a1587367767b\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "With our introduction complete, let's dive into the practical tutorial!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mQjwWu4XzgG"
      },
      "source": [
        "---\n",
        "        \n",
        "üí° **NOTE**: We will be using together.ai and the remote LLM.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJe5rT-ChHWe"
      },
      "source": [
        "We will start by installing a number of packages that we are going to use throughout this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EB89XRAFXxE"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic datasets -qqq\n",
        "\n",
        "# DataMapPlot\n",
        "!pip install datamapplot -q\n",
        "\n",
        "# GPU-accelerated HDBSCAN + UMAP\n",
        "!pip install cudf-cu12 dask-cudf-cu12 --extra-index-url=https://pypi.nvidia.com -q\n",
        "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com -q\n",
        "!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com -q\n",
        "!pip install cupy-cuda12x -f https://pip.cupy.dev/aarch64 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q83Pinrlcqe"
      },
      "outputs": [],
      "source": [
        "from bertopic.representation import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oscg1dan-mWE"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAxVj9cGYWX1"
      },
      "source": [
        "# üìÑ **Data**\n",
        "\n",
        "We are going to apply topic modeling on a number of Patent abstracts. They are a great source for topic modeling since they contain a wide variety of technologies and therefore topics and are generally well-written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwXAMn1D-T3k"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbwHbJc_YXi8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"RJuro/neuro_patents\")['train']\n",
        "\n",
        "# Extract abstracts to train on and corresponding titles\n",
        "abstracts = dataset[\"appln_abstract\"]\n",
        "titles = dataset[\"appln_title\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X93aomFFhYvd"
      },
      "source": [
        "To give you an idea, an abstract looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITk9qOOWhd61"
      },
      "outputs": [],
      "source": [
        "# a sleeping stage monitor\n",
        "print(textwrap.fill(abstracts[8765], width=80))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3kCbYj9mqRI"
      },
      "outputs": [],
      "source": [
        "len(abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g7dXXjmY9Um"
      },
      "source": [
        "# üí¨ **Utilizing the OpenAI Package with together.ai API**\n",
        "\n",
        "In this tutorial segment, we'll explore the utilization of the OpenAI package, leveraging its compatibility with the together.ai API for integrating Large Language Models (LLMs). Specifically, we will focus on the Mixtral model from Mistral, which represents an optimal balance of performance (surpasses chatGPT3.5) and computational efficiency - and also price!\n",
        "\n",
        "### Integration Steps:\n",
        "\n",
        "1. **OpenAI Client Setup**: We initiate by configuring the OpenAI client. This setup involves specifying the together.ai API as the base URL and providing the necessary API key. The compatibility with the OpenAI format simplifies this process, allowing for a smooth integration. And later you can swap out for chatGPT or a local LLM.\n",
        "\n",
        "2. **Model Selection and Request**: Our model of choice for this task is the Mixtral model from Mistral. Through the OpenAI package, we will craft a request that aligns with the together.ai API specifications. This step includes defining the task, such as text classification or generation, and setting parameters like temperature for controlling the output's creativity.\n",
        "\n",
        "3. **Execution and Output Retrieval**: With the request formulated, we execute it via the OpenAI client, which communicates with the together.ai API backend. The backend, understanding the OpenAI format, efficiently processes our request using the specified Mixtral model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ijpxZZBAj8A"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hndz9el-r0n"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLL9yv2MAdgt"
      },
      "outputs": [],
      "source": [
        "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5EjzCO4-tDS"
      },
      "outputs": [],
      "source": [
        "# Point to the local server\n",
        "client = openai.OpenAI(base_url=\"https://api.together.xyz/v1\", api_key=TOGETHER_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvySvTeHTyLZ"
      },
      "source": [
        "## **Prompt Engineering**\n",
        "\n",
        "To check whether our model is correctly loaded, let's try it out with a few prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMqzTgk-mabE"
      },
      "outputs": [],
      "source": [
        "system = \"You are a helpful assistant\"\n",
        "user = \"Could you explain to me training dogs works as if I am 5?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-oyAEyyAW1B"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", # this field is currently unused\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": user}\n",
        "  ],\n",
        "  temperature=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXNG3SNHAsPW"
      },
      "outputs": [],
      "source": [
        "print(textwrap.fill(completion.choices[0].message.content, width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck2eMBn8T0Ef"
      },
      "outputs": [],
      "source": [
        "# let's print the markdown produced, too\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(completion.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1yfA9q_sCo8"
      },
      "source": [
        "### **Prompt Template**\n",
        "\n",
        "We are going to keep our `system prompt` simple and to the point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-hE4dWisJl6"
      },
      "outputs": [],
      "source": [
        "# System prompt describes information given to all conversations\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful, respectful and honest assistant for labeling scientific and technical topics - particularly withing neuroscience and neurotech.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUEbL6C_u_ii"
      },
      "source": [
        "We will tell the model that it is simply a helpful assistant for labeling topics since that is our main goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp8ug7FnsIpv"
      },
      "source": [
        "In contrast, our `user prompt` is going to the be a bit more involved. It will consist of two components, an **example** and the **main prompt**.\n",
        "\n",
        "Let's start with the **example**. Most LLMs do a much better job of generating accurate responses if you give them an example to work with. We will show it an accurate example of the kind of output we are expecting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEBdQ8xasg_x"
      },
      "outputs": [],
      "source": [
        "# Example prompt demonstrating the output we are looking for\n",
        "example_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "- Optogenetics allows for the control of specific neurons with high temporal precision using light, making it a powerful tool for studying neural circuits.\n",
        "- Recent developments in optogenetic tools have improved the targeting of specific cell types, enhancing the ability to manipulate neural pathways with minimal invasiveness.\n",
        "- The combination of optogenetics with other techniques, such as electrophysiology, provides deeper insights into the functional dynamics of neural networks.\n",
        "\n",
        "The topic is described by the following keywords: 'optogenetics, neurons, neural circuits, light, cell types, neural pathways, electrophysiology'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
        "\"\"\"\n",
        "\n",
        "example_output = \"\"\"Advancements in optogenetics for precise neural circuit manipulation\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjZrqwxvUzS"
      },
      "source": [
        "This example, based on a number of keywords and documents primarily about the impact of\n",
        "meat, helps to model to understand the kind of output it should give. We show the model that we were expecting only the label, which is easier for us to extract.\n",
        "\n",
        "Next, we will create a template that we can use within BERTopic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnhOj7mnTw5f"
      },
      "outputs": [],
      "source": [
        "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
        "main_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdEc9BJYvnhI"
      },
      "source": [
        "There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]` and `[KEYWORDS]`:\n",
        "\n",
        "* `[DOCUMENTS]` contain the top 5 most relevant documents to the topic\n",
        "* `[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated through c-TF-IDF\n",
        "\n",
        "This template will be filled accordingly to each topic. And finally, we can combine this into our final prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhySKdfWwBTz"
      },
      "outputs": [],
      "source": [
        "prompt = system_prompt + example_prompt + main_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm8xdi2BXdDK"
      },
      "outputs": [],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mDfAxLbYaeU"
      },
      "source": [
        "# üó®Ô∏è **BERTopic**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWBeqL60NWu"
      },
      "source": [
        "Before we can start with topic modeling, we will first need to perform two steps:\n",
        "* Pre-calculating Embeddings\n",
        "* Defining Sub-models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HSPV-N9Ydp1"
      },
      "source": [
        "## **Preparing Embeddings**\n",
        "\n",
        "By pre-calculating the embeddings for each document, we can speed-up additional exploration steps and use the embeddings to quickly iterate over BERTopic's hyperparameters if needed.\n",
        "\n",
        "use `BAAI/bge-small-zh-v1.5`for Chinese.\n",
        "\n",
        "\n",
        "üî• **TIP**: You can find a great overview of good embeddings for clustering on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqhLlG8_YhtB"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEwb6TkzYgBs"
      },
      "source": [
        "## **Sub-models**\n",
        "\n",
        "Next, we will define all sub-models in BERTopic and do some small tweaks to the number of clusters to be created, setting random states, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL3oQPkJZQqF"
      },
      "outputs": [],
      "source": [
        "from cuml.manifold import UMAP\n",
        "from cuml.cluster import HDBSCAN\n",
        "# from umap import UMAP\n",
        "# from hdbscan import HDBSCAN\n",
        "\n",
        "umap_model = UMAP(n_neighbors=3, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=25, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1FyIXxHgrmj"
      },
      "source": [
        "As a small bonus, we are going to reduce the embeddings we created before to 2-dimensions so that we can use them for visualization purposes when we have created our topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4vl5olHgjXe"
      },
      "outputs": [],
      "source": [
        "# Pre-reduce embeddings for visualization purposes\n",
        "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zon-88JX-0os"
      },
      "source": [
        "### **Representation Models**\n",
        "\n",
        "One of the ways we are going to represent the topics is with Zephyr which should give us a nice label. However, we might want to have additional representations to view a topic from multiple angles.\n",
        "\n",
        "Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired), [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance), and [Zephyr](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html) as our additional representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wRtJdHZEMjM"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "I have a topic that is described by the following keywords: [KEYWORDS]\n",
        "In this topic, the following documents are a small but representative subset of all documents in the topic:\n",
        "[DOCUMENTS]\n",
        "\n",
        "Based on the information above, please give a topic label of maximum 6 words:\n",
        "topic: <label>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tgJLEBoZU2U"
      },
      "outputs": [],
      "source": [
        "#from bertopic.representation import OpenAI\n",
        "\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "\n",
        "# KeyBERT\n",
        "keybert = KeyBERTInspired()\n",
        "\n",
        "# MMR\n",
        "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "#openai_rep = OpenAI(client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "#                    chat=True,\n",
        "#                    prompt=prompt,\n",
        "#                    nr_docs=5,\n",
        "#                    delay_in_seconds=3)\n",
        "\n",
        "\n",
        "# All representation models\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert,\n",
        "#    \"Llama\": openai_rep,\n",
        "    \"MMR\": mmr,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLVYJQf2aegN"
      },
      "source": [
        "# üî• **Training**\n",
        "\n",
        "Now that we have our models prepared, we can start training our topic model! We supply BERTopic with the sub-models of interest, run `.fit_transform`, and see what kind of topics we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdokcXzW6C1g"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(\n",
        "\n",
        "  # Sub-models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  representation_model=representation_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=15,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VP3thw16ECx"
      },
      "source": [
        "Now that we are done training our model, let's see what topics were generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uj8MYhCafmX"
      },
      "outputs": [],
      "source": [
        "# Show topics\n",
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axujqkR4sQZq"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(1, full=True)[\"KeyBERT\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WORKAROUND ###\n",
        "\n",
        "unfortunately there are some incompatibilities right now. I needed to implement the labeling by hand"
      ],
      "metadata": {
        "id": "Bo9OG-d5rGwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# System prompt describing the context\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful, respectful and honest assistant for labeling scientific and technical topics - particularly within neuroscience and neurotech.\n",
        "\"\"\"\n",
        "\n",
        "# Example prompt demonstrating the desired output\n",
        "example_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "- Optogenetics allows for the control of specific neurons with high temporal precision using light, making it a powerful tool for studying neural circuits.\n",
        "- Recent developments in optogenetic tools have improved the targeting of specific cell types, enhancing the ability to manipulate neural pathways with minimal invasiveness.\n",
        "- The combination of optogenetics with other techniques, such as electrophysiology, provides deeper insights into the functional dynamics of neural networks.\n",
        "\n",
        "The topic is described by the following keywords: 'optogenetics, neurons, neural circuits, light, cell types, neural pathways, electrophysiology'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
        "\"\"\"\n",
        "\n",
        "example_output = \"Advancements in optogenetics for precise neural circuit manipulation\"\n",
        "\n",
        "# Our main prompt template\n",
        "main_prompt_template = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "{documents}\n",
        "\n",
        "The topic is described by the following keywords: '{keywords}'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# List to store the results\n",
        "labels = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for index, row in tqdm(topic_model.get_topic_info().iterrows(), total=topic_model.get_topic_info().shape[0], desc=\"Labeling Topics\"):\n",
        "    documents = row['Representative_Docs']\n",
        "    keywords = row['KeyBERT']\n",
        "\n",
        "    # Create the main prompt for the current topic\n",
        "    main_prompt = main_prompt_template.format(documents=\"\\n\".join(documents), keywords=keywords)\n",
        "\n",
        "    # Combine system, example, and main prompts\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": example_output},\n",
        "        {\"role\": \"user\", \"content\": main_prompt},\n",
        "    ]\n",
        "\n",
        "    # Call the LLM model to generate the label\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",  # Specified model, currently unused\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    # Extract the label from the response and add it to the results list\n",
        "    label = completion.choices[0].message.content.strip()\n",
        "    labels.append(label)\n",
        "\n",
        "# 'labels' now contains the topic labels generated for each topic in the dataframe"
      ],
      "metadata": {
        "id": "oyAa6NqrrNHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg9rglMro2sZ"
      },
      "source": [
        "We got over 100 topics that were created and they all seem quite diverse.We can use the labels by Llama 2 and assign them to topics that we have created. Normally, the default topic representation would be c-TF-IDF, but we will focus on Llama 2 representations instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ufudzoUZzH"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, hide_annotations=True, hide_document_hover=False, custom_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-V5KG4LKqYi"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Prepare logo\n",
        "bertopic_logo_response = requests.get(\n",
        "    \"https://raw.githubusercontent.com/MaartenGr/BERTopic/master/images/logo.png\",\n",
        "    stream=True,\n",
        "    headers={'User-Agent': 'My User Agent 1.0'}\n",
        ")\n",
        "bertopic_logo = np.asarray(PIL.Image.open(bertopic_logo_response.raw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCxzuQ4eKtId"
      },
      "outputs": [],
      "source": [
        "import datamapplot\n",
        "import re\n",
        "\n",
        "# Create a label for each document | notice that we are passing labels from manual labeling here\n",
        "llm_labels = [label if label else \"Unlabelled\" for label in labels]\n",
        "all_labels = [llm_labels[topic+topic_model._outliers] if topic != -1 else \"Unlabelled\" for topic in topics]\n",
        "\n",
        "# Run the visualization\n",
        "datamapplot.create_plot(\n",
        "    reduced_embeddings,\n",
        "    all_labels,\n",
        "    label_font_size=10,\n",
        "    title=\"Neurotech - Topics\",\n",
        "    sub_title=\"Topics labeled with `Llama 3.1`\",\n",
        "    label_wrap_width=20,\n",
        "    use_medoids=True,\n",
        "    #logo=bertopic_logo,\n",
        "    #logo_width=0.16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b78XgyUIVp1"
      },
      "outputs": [],
      "source": [
        "dataset.to_parquet('patents_with_tm.pq')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}