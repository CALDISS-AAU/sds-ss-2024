{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dimensionality Reduction and Clustering\n",
        "## Nomad Cities around the World\n",
        "\n",
        "In this tutorial we are going to play with dimensionality reduction and clustering using a nomadlist dataset. The data describes 780 cities around the world and includes variables interesting for nomads traveling to these destinations.\n",
        "Features include: internet speed, cost variables and socio-political indicators."
      ],
      "metadata": {
        "id": "1yPKUL9lIpSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading data and libraries"
      ],
      "metadata": {
        "id": "J7FBVhCsJjyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn -q"
      ],
      "metadata": {
        "id": "L3oob5R0XRWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h5tH7rBPkLI"
      },
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2M7SgFLP6EA"
      },
      "source": [
        "# reading in the data\n",
        "data = pd.read_csv('https://sds-aau.github.io/SDS-master/M1/data/cities.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, we already included some easier to work with geo-variables (e.g. country-code, region and sub-region)"
      ],
      "metadata": {
        "id": "nxCcIxLWJW0k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgAi-USQB7Z"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlTuEAWQRJj"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing for UML\n",
        "\n",
        "Typical pre-processing steps for UML include different forms of scaling. This is similar to supervised approaches.\n",
        "- Standard scaling: Data will have a mean of 0 and œÉ of 1\n",
        "- Min-max scaling: Features are scaled to a range, typically 0, 1"
      ],
      "metadata": {
        "id": "PR0qkVacJig5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47yX2obQjBv"
      },
      "source": [
        "# We select only numerical features from the dataframe\n",
        "# naming is in anticipation of future clustering\n",
        "data_to_cluster = data.iloc[:,4:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import and instantiate scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "tO6jU9VIKRmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8e0cB8vRJ0y"
      },
      "source": [
        "# learn x-y relationships (principal components) and transform\n",
        "data_to_cluster_scaled = scaler.fit_transform(data_to_cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_25WxpDKRd5H"
      },
      "source": [
        "# very similar syntax for min-max scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_min_max = MinMaxScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_cluster_minmax = scaler_min_max.fit_transform(data_to_cluster)"
      ],
      "metadata": {
        "id": "yiWa6EOpLBUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_cluster"
      ],
      "metadata": {
        "id": "mNtb1XAALuXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check how our data look pre/post scaling"
      ],
      "metadata": {
        "id": "WF2UD7_2MZtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
        "\n",
        "# nomad-cost (pre-scaling)\n",
        "sns.kdeplot(data=data_to_cluster, x=\"cost_nomad\", ax=axes[0, 0])\n",
        "axes[0, 0].set_title(\"Nomad Cost (pre-scaling)\")\n",
        "\n",
        "# coffee (pre-scaling)\n",
        "sns.kdeplot(data=data_to_cluster, x=\"coffee_in_cafe\", ax=axes[0, 1])\n",
        "axes[0, 1].set_title(\"Coffee in Cafe (pre-scaling)\")\n",
        "\n",
        "# convert scaled data to dataframe\n",
        "scaled_df = pd.DataFrame(data_to_cluster_scaled, columns=data_to_cluster.columns)\n",
        "\n",
        "# nomad-cost (post-scaling)\n",
        "sns.kdeplot(data=scaled_df, x=\"cost_nomad\", ax=axes[1, 0])\n",
        "axes[1, 0].set_title(\"Nomad Cost (post-scaling)\")\n",
        "\n",
        "# coffee (post-scaling)\n",
        "sns.kdeplot(data=scaled_df, x=\"coffee_in_cafe\", ax=axes[1, 1])\n",
        "axes[1, 1].set_title(\"Coffee in Cafe (post-scaling)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MTzOnRQixAUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality reduction with PCA\n",
        "\n",
        "PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. (Source: Wikipedia)"
      ],
      "metadata": {
        "id": "eMhyL1oMRpxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Stitch Fix is using something called eigenvector decomposition, a concept from quantum mechanics, to tease apart the overlapping ‚Äúnotes‚Äù in an individual‚Äôs style. Using physics, the team can better understand the complexities of the clients‚Äô style minds. <a href=\"https://t.co/iULGyYsd5c\">https://t.co/iULGyYsd5c</a></p>&mdash; WIRED (@WIRED) <a href=\"https://twitter.com/WIRED/status/1181437300414275584?ref_src=twsrc%5Etfw\">October 8, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ],
      "metadata": {
        "id": "hw0DKgxuSdCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a deep dive into PCA, please consider [this chapter](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)"
      ],
      "metadata": {
        "id": "qx5Db18sKwjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) primarily focuses on identifying and quantifying the underlying patterns in the data. It seeks to explain variance-covariance (or correlation) structures from linear combinations of the initial variables.\n",
        "\n",
        "Mathematically, for a data matrix $X$ (where rows are observations and columns are features), the main steps in PCA are:\n",
        "1. Standardize the data (subtract the mean, devide by standard deviation).\n",
        "2. Calculate the covariance matrix.\n",
        "3. Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
        "4. Sort the eigenvectors by decreasing eigenvalues and choose the first $k$ eigenvectors to form a matrix $W$ of dimensions $(features \\times k)$.\n",
        "5. Project the data onto $W$ to get the principal components.\n",
        "\n",
        "Given that, let's compute the covariance matrix for our scaled data and find its eigenvalues and eigenvectors:\n"
      ],
      "metadata": {
        "id": "y9bay_N9RaaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 3D data\n",
        "sim_data = np.array([[2.5, 2.4, 2.1], [0.5, 0.7, 0.2], [2.2, 2.9, 2.3], [1.9, 2.2, 1.8], [3.1, 3.0, 2.8]])\n",
        "sim_data.shape"
      ],
      "metadata": {
        "id": "UacKn8KKRtBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Standardize the dataset (mean = 0, variance = 1)\n",
        "X_mean = np.mean(sim_data, axis=0)\n",
        "X_std = np.std(sim_data, axis=0)\n",
        "X_normalized = (sim_data - X_mean) / X_std"
      ],
      "metadata": {
        "id": "1E1BMdjSTDR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Compute covariance matrix of the standardized dataset\n",
        "covariance_matrix = np.cov(X_normalized.T)"
      ],
      "metadata": {
        "id": "1ssGK7tMTWfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interlude: Covariance matrix from scratch\n",
        "\n",
        "Suppose we have two variables, $X$ and $Y$, with the following data points:\n",
        "\n",
        "$$ X = [2, 4, 6] $$\n",
        "$$ Y = [3, 6, 9] $$\n",
        "\n",
        "### Step-by-step Calculation:\n",
        "\n",
        "1. **Compute the mean of each variable:**\n",
        "   $$ \\mu_X = \\frac{\\sum{X}}{n} $$\n",
        "   $$ \\mu_Y = \\frac{\\sum{Y}}{n} $$\n",
        "  \n",
        "2. **Compute the products of the differences from the mean for each data point:**\n",
        "   $$ (x_i - \\mu_X)(y_i - \\mu_Y) $$\n",
        "\n",
        "3. **Compute the covariance for each pair of variables:**\n",
        "   $$ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum{(x_i - \\mu_X)(y_i - \\mu_Y)} $$\n",
        "\n",
        "4. **Populate the covariance matrix:**\n",
        "   The covariance matrix for variables $X$ and $Y$ is:\n",
        "   \n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   \\text{Var}(X) & \\text{Cov}(X,Y) \\\\\n",
        "   \\text{Cov}(Y,X) & \\text{Var}(Y) \\\\\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   Where:\n",
        "   - $\\text{Var}(X)$ is the variance of $X$ = $\\text{Cov}(X, X)$\n",
        "   - $\\text{Var}(Y)$ is the variance of $Y$ = $\\text{Cov}(Y, Y)$\n"
      ],
      "metadata": {
        "id": "CxooRIYNWMbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## INTERLUDE - covmatrix by hand\n",
        "\n",
        "# Given data points\n",
        "X = np.array([2, 4, 6])\n",
        "Y = np.array([3, 6, 9])\n",
        "\n",
        "# Means of X and Y\n",
        "mu_X = np.mean(X)\n",
        "mu_Y = np.mean(Y)\n",
        "\n",
        "# Compute Covariance\n",
        "cov_XY = np.sum((X - mu_X) * (Y - mu_Y)) / (len(X) - 1)\n",
        "var_X = np.sum((X - mu_X)**2) / (len(X) - 1)\n",
        "var_Y = np.sum((Y - mu_Y)**2) / (len(Y) - 1)\n",
        "\n",
        "# Covariance Matrix\n",
        "cov_matrix = np.array([[var_X, cov_XY], [cov_XY, var_Y]])\n",
        "print(cov_matrix)\n"
      ],
      "metadata": {
        "id": "i17TUAhLTpVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check...if we did OK ## INTERLUDE OVER\n",
        "np.cov(np.array([[2,4,6],[3,6,9]]))"
      ],
      "metadata": {
        "id": "xNSRFvGAWjVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Obtain the eigenvectors and eigenvalues\n",
        "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)"
      ],
      "metadata": {
        "id": "7m-FM9XPW7jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by eigenvalue in descending order\n",
        "sorted_idx = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvectors = eigenvectors[:, sorted_idx]"
      ],
      "metadata": {
        "id": "V3sYWdHxXglA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Reduce dimensions by selecting top 'num_components' eigenvectors\n",
        "num_components = 2\n",
        "reduced_eigenvectors = sorted_eigenvectors[:, :num_components]"
      ],
      "metadata": {
        "id": "fnRAqJaeX3oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dot-product or Matrix Multiplication\n",
        "\n",
        "Matrix multiplication, also known as the dot product, is an operation that takes two matrices, A and B, and produces another matrix, C. The number of columns in matrix A must be equal to the number of rows in matrix B to be able to multiply them. The resulting matrix C has a size determined by the number of rows in matrix A and the number of columns in matrix B.\n",
        "\n",
        "For a clearer and interactive visualization on how each element of the resulting matrix is computed, visit [this link](http://matrixmultiplication.xyz/).\n"
      ],
      "metadata": {
        "id": "P4EC19N3qcJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Transform the dataset to the new subspace\n",
        "reduced_data_scratch = X_normalized.dot(reduced_eigenvectors)"
      ],
      "metadata": {
        "id": "fOhtf7ApYPej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_data_scratch"
      ],
      "metadata": {
        "id": "n3MsgohUYncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OR if using Sklearn\n",
        "\n",
        "# 1. Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "sim_data_standardized = scaler.fit_transform(sim_data)\n",
        "\n",
        "# 2. Create a PCA instance and fit\n",
        "num_components = 2\n",
        "pca = PCA(n_components=num_components)\n",
        "pca.fit(sim_data_standardized)\n",
        "\n",
        "# 3. Transform the original data to the new subspace\n",
        "reduced_data_sklearn = pca.transform(sim_data_standardized)"
      ],
      "metadata": {
        "id": "H7L00E5BYWqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_data_sklearn"
      ],
      "metadata": {
        "id": "ME0Y0v5qYw1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's apply on our example data"
      ],
      "metadata": {
        "id": "3FMA-2S7ylk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load up and instantiate PCA\n",
        "pca = PCA(n_components=2)"
      ],
      "metadata": {
        "id": "MlHgVv83IQnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit-transform the data\n",
        "data_reduced_pca = pca.fit_transform(data_to_cluster_scaled)\n",
        "pca.components_.shape"
      ],
      "metadata": {
        "id": "TzojR3TcM8XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "MIJQYkJMNtu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now plot the reduced data\n",
        "sns.scatterplot(x=data_reduced_pca[:,0], y=data_reduced_pca[:,1])"
      ],
      "metadata": {
        "id": "mRFLn0WtOEjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a more informative plot using altair and bringing some data back into the picture"
      ],
      "metadata": {
        "id": "KujKyWDVOioB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame based on the reduced data from PCA\n",
        "vis_data = pd.DataFrame(data_reduced_pca)\n",
        "\n",
        "# Add 'place' column from the original 'data' DataFrame to 'vis_data'\n",
        "vis_data['place'] = data['place']\n",
        "\n",
        "# Add 'country' column, represented by its alpha-2 code, from the original 'data' DataFrame to 'vis_data'\n",
        "vis_data['country'] = data['alpha-2']\n",
        "\n",
        "# Rename the columns of 'vis_data' for better clarity:\n",
        "# The first two columns represent the two principal components from PCA\n",
        "# The third and fourth columns are 'place' and 'country' respectively\n",
        "vis_data.columns = ['x', 'y', 'place', 'country']\n",
        "\n",
        "# Using the Altair library to create an interactive scatter plot:\n",
        "# - The x and y axes represent the two principal components.\n",
        "# - Each data point (or circle) in the scatter plot corresponds to a 'place' in a 'country'.\n",
        "# - Hovering over a data point reveals a tooltip with the 'place' and 'country' information.\n",
        "alt.Chart(vis_data).mark_circle(size=60).encode(\n",
        "    x='x',          # Set the x-axis to represent the first principal component\n",
        "    y='y',          # Set the y-axis to represent the second principal component\n",
        "    tooltip=['place', 'country']  # Display 'place' and 'country' information as a tooltip on hover\n",
        ").interactive()   # Enable interactive features such as panning and zooming\n"
      ],
      "metadata": {
        "id": "CCVp5_B5zYuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,2))\n",
        "sns.heatmap(pd.DataFrame(pca.components_, columns=data_to_cluster.columns), annot=True)"
      ],
      "metadata": {
        "id": "6_VEMHhVNabH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From looking at the components, we can \"see\" that while the 1st is capturing political features (i.e. freedom and fragility), the second is bringing together all cost-variables (that are correlated)"
      ],
      "metadata": {
        "id": "b6LcGLskP24l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#quick correlation check\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = data_to_cluster.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "metadata": {
        "id": "aFu1i21eNaU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also very much recommend you to follow [this tutorial](https://youtu.be/52d7ha-GdV8) where you will learn to implement PCA starting out with the math and building your own module."
      ],
      "metadata": {
        "id": "uyZHayAqUWG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/52d7ha-GdV8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "metadata": {
        "id": "pcrsLqTnUdpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality Reduction with NMF\n",
        "\n",
        "NMF is another popular dimensinality reduction technique based on matrix-decomposition. One advantage here is that components are often \"more equal\" in their importance. It is a more modern technique and is often very good at capturing latent paterns in data.\n",
        "\n",
        "The number of components is a bit of a \"debated issue\" and as with many things in UML (that is also debatable) more of a choice of the analyst."
      ],
      "metadata": {
        "id": "-FQ89RDyRnIJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U91nOXzNR_9Z"
      },
      "source": [
        "# import nmf\n",
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axawIZYJSY0d"
      },
      "source": [
        "# instantiate with 4 components\n",
        "nmf = NMF(n_components=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRdddhIJTA7H"
      },
      "source": [
        "# notice, we are using the min-max scaled data\n",
        "data_reduced_nmf = nmf.fit_transform(data_to_cluster_minmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAuYKjNAT7aj"
      },
      "source": [
        "data_reduced_nmf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBiu4kLuT-3R"
      },
      "source": [
        "nmf.components_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,3))\n",
        "sns.heatmap(pd.DataFrame(nmf.components_, columns=data_to_cluster.columns), annot=True)"
      ],
      "metadata": {
        "id": "VuPCgnZbWha6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moving into more modern algorithms\n",
        "\n",
        "In the recent years more advanced algos evolved and are being used for dimensionality reduction and visualization. t-SNE was popular around 2016 but then \"replaced\" by UMAP."
      ],
      "metadata": {
        "id": "i3pRlfamXMTS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9u-3dYGUJjz"
      },
      "source": [
        "import umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZmMaKo3Vghp"
      },
      "source": [
        "# we totally could specify more than 2 dimensions (as well as some other parameters)\n",
        "umap_scaler = umap.UMAP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEbq8m40Uk7E"
      },
      "source": [
        "# umap accepts standard-scaled data\n",
        "embeddings = umap_scaler.fit_transform(data_to_cluster_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "umap reduced data is often called \"embeddings\" which brings it terminology-wise closer to deep learning approaches. Probably this is because it is sometimes used in combination with modern NLP techniques like SBERT."
      ],
      "metadata": {
        "id": "FQW0Ur4NZnYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just as PCA, umap reduced data can be plottet\n",
        "sns.scatterplot(x=embeddings[:,0], y=embeddings[:,1])"
      ],
      "metadata": {
        "id": "EeBIR6xyIgfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Umap combines global and local features for deminsionality reduction with axis representing a combination of features that often align well with \"human intuation\" about data."
      ],
      "metadata": {
        "id": "UxOMgsQSaKuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a new DataFrame from the embeddings and merge with 'place' and 'country' columns from the original data\n",
        "vis_data = pd.DataFrame({\n",
        "    'x': embeddings[:, 0],         # Assuming embeddings is a 2D array or similar structure\n",
        "    'y': embeddings[:, 1],\n",
        "    'place': data['place'],\n",
        "    'country': data['alpha-2']\n",
        "})\n",
        "\n",
        "# Create an interactive scatter plot using Altair\n",
        "chart = alt.Chart(vis_data).mark_circle(size=60).encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    tooltip=['place', 'country']\n",
        ").interactive()\n",
        "\n",
        "chart\n"
      ],
      "metadata": {
        "id": "qK4fakKH0k_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Similar to dimensionality reduction, clustering aims at identifying latent patterns in the data. In addition, clustering algorithms sort data into (simetimes) predefined clusters.\n",
        "\n",
        "There exist many different approaches to clustering. One of the most used ones is K-means.\n",
        "\n",
        "For a deep-dive, consider [this chapter](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
        "\n",
        "Consider also [this tutorial](https://youtu.be/vtuH4VRq1AU) where you learn how to implement the algorithm from scratch (starting with the math) in Python."
      ],
      "metadata": {
        "id": "mK9veCbCc1WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Dive into K-means Clustering\n",
        "\n",
        "K-means clustering aims to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean. The algorithm works iteratively to assign each data point to one of the $k$ groups based on the features provided.\n",
        "\n",
        "Mathematically, the primary objective is to minimize:\n",
        "\n",
        "$$\\ J = \\sum_{i=1}^{k} \\sum_{x \\in S_i} ||x - \\mu_i||^2 \\$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $J$ is the objective function,\n",
        "- $x$ is a data point in cluster $S_i$,\n",
        "- $\\mu_i$ is the centroid of $S_i$.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. Initialize the $k$ cluster centroids (randomly pick samples from the data as initial centroids).\n",
        "2. Assign each data point to the closest centroid.\n",
        "3. Recompute the centroids based on the current cluster assignments.\n",
        "4. Repeat steps 2 and 3 until the assignments do not change or a maximum number of iterations is reached.\n",
        "\n",
        "Let's write a simple implementation of the k-means clustering for a clearer understanding:\n"
      ],
      "metadata": {
        "id": "JeJXG41wtqUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters=100\n",
        "k = 3"
      ],
      "metadata": {
        "id": "J6gZ2FwMrAh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the k cluster centroids\n",
        "centroids = data_reduced_pca[np.random.choice(data.shape[0], k, replace=False)]"
      ],
      "metadata": {
        "id": "g368vRvUr5XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot observations\n",
        "sns.scatterplot(x=data_reduced_pca[:, 0], y=data_reduced_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=centroids[:, 0], y=centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and Initial Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SqUBF_IUspEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Assign each data point to the closest centroid\n",
        "distances = np.linalg.norm(data_reduced_pca - centroids[:, np.newaxis], axis=2)\n",
        "labels = np.argmin(distances, axis=0)"
      ],
      "metadata": {
        "id": "Jas4-LQAtPRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Recompute the centroids\n",
        "new_centroids = np.array([data_reduced_pca[labels == i].mean(axis=0) for i in range(k)])"
      ],
      "metadata": {
        "id": "wErB_GW2tvO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_centroids"
      ],
      "metadata": {
        "id": "eOfNUe4NvBHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot observations after 1st interation\n",
        "sns.scatterplot(x=data_reduced_pca[:, 0], y=data_reduced_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=new_centroids[:, 0], y=new_centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and 1st interation Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cK5eBOBtt1Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple implementatino of K-means\n",
        "\n",
        "def k_means_simple(data, k, max_iters=100):\n",
        "    # 1. Initialize the k cluster centroids\n",
        "    centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        # 2. Assign each data point to the closest centroid\n",
        "        distances = np.linalg.norm(data - centroids[:, np.newaxis], axis=2)\n",
        "        labels = np.argmin(distances, axis=0)\n",
        "\n",
        "        # 3. Recompute the centroids\n",
        "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels, centroids\n"
      ],
      "metadata": {
        "id": "gWaWAfKqtlqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test our simple k-means\n",
        "labels, final_centroids = k_means_simple(data_reduced_pca, 3)\n",
        "print(\"Cluster centroids:\\n\", final_centroids)"
      ],
      "metadata": {
        "id": "nj0iAsrcuIbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot observations after 100st interation\n",
        "sns.scatterplot(x=data_reduced_pca[:, 0], y=data_reduced_pca[:, 1], alpha=0.6, color='blue')\n",
        "\n",
        "# Plot centroids\n",
        "sns.scatterplot(x=final_centroids[:, 0], y=final_centroids[:, 1], color='red', s=100)\n",
        "\n",
        "plt.title('PCA Reduced Data and last interation Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FM5s_JclubCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = KMeans(n_clusters=3)"
      ],
      "metadata": {
        "id": "8bsuE_D8c1IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of clusters is a bit of a hot topic.\n",
        "The \"elbow method\" is still widely used for \"estimating\" the optimal number. It looks at _inertia_ as a measure of clustering quality and suggest to use the number where an \"elbow\" can be seen when plotting inertia values for different $n\\_cluster$."
      ],
      "metadata": {
        "id": "Yzy-it_Ed_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an empty list to store the sum of squared distances for each 'k'\n",
        "Sum_of_squared_distances = []\n",
        "\n",
        "# Define a range for possible cluster values (1 to 9)\n",
        "K = range(1, 10)\n",
        "\n",
        "# For each possible 'k', fit a KMeans model and compute the sum of squared distances\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k, n_init = \"auto\")               # Initialize the KMeans model with 'k' clusters\n",
        "    km.fit(data_to_cluster_scaled)          # Fit the model on the scaled data\n",
        "    Sum_of_squared_distances.append(km.inertia_)  # Append the model's inertia (sum of squared distances) to the list\n"
      ],
      "metadata": {
        "id": "EgJ205Uv1N5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the sum of squared distances for each 'k' to determine the 'elbow'\n",
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Sum of Squared Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.grid(True)  # Add a grid for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DRE7pvDa1P_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the optimal number of clusters is often a subject of debate. The \"elbow method\" remains a popular technique to estimate this number. This method evaluates the _inertia_ (sum of squared distances) as a metric for clustering quality. By plotting inertia values against varying cluster counts, we look for an \"elbow\" point. This \"elbow\" typically suggests the optimal number of clusters where adding more doesn't provide significant better fit to the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ckAmpmEbfRYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umap_scaler_km = umap.UMAP(n_components=6)\n",
        "embeddings_km = umap_scaler.fit_transform(data_to_cluster_scaled)\n",
        "\n",
        "\n",
        "Sum_of_squared_distances = []\n",
        "K = range(1,10)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k, n_init = \"auto\")\n",
        "    km = km.fit(embeddings_km)\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4yn4_tFAc0yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Le5qhRkENM"
      },
      "source": [
        "# back to our k-means instance. We take 3 clusters on non-reduced data\n",
        "clusterer.fit(data_to_cluster_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ1mO2Fdkjyn"
      },
      "source": [
        "# we can then copy the cluster-numbers into the original file and start exploring\n",
        "data['cluster'] = clusterer.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEmm7S0Pk9p-"
      },
      "source": [
        "# e.g. which cluster seems most lgbt-friendly üåà\n",
        "data.groupby('cluster').lgbt_friendly.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g. which cluster seems most party-places ü•≥\n",
        "data.groupby('cluster').nightlife.mean()"
      ],
      "metadata": {
        "id": "opEEZZnAeUSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine clustering with our UMAP embeedings in the viz."
      ],
      "metadata": {
        "id": "TTv5xhPMjawa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vis_data = pd.DataFrame(embeddings)\n",
        "vis_data['place'] = data['place']\n",
        "vis_data['cluster'] = data['cluster']\n",
        "vis_data['country'] = data['alpha-2']\n",
        "vis_data.columns = ['x', 'y', 'place', 'cluster','country']"
      ],
      "metadata": {
        "id": "v3G8-CDxjYN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.Chart(vis_data).mark_circle(size=60).encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    tooltip=['place', 'country'],\n",
        "    color=alt.Color('cluster:N', scale=alt.Scale(scheme='dark2')) #use N after the var to tell altair that it's categorical\n",
        ").interactive()"
      ],
      "metadata": {
        "id": "uHPT8tJwjYN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity and Distance in Recommendations\n",
        "\n",
        "Recommendation systems often deploy two key methodologies:\n",
        "\n",
        "1. **Content-Based Recommendation:** Based on underlying characteristics or properties of items. For instance, recommending similar products or content. This method often employs principles from unsupervised machine learning (UML).\n",
        "   \n",
        "2. **Collaborative Filtering:** Leverages behavioral patterns of users to recommend items. This is based on the idea of \"users similar to you also liked...\"\n",
        "\n",
        "The foundation of many recommendation approaches is determining how \"similar\" or \"distant\" items or users are from one another.\n",
        "\n",
        "### Euclidean Distance\n",
        "\n",
        "Euclidean Distance is a widely used metric to calculate similarity in the context of vectors.\n",
        "\n",
        "![Euclidean Distance Visualization](https://upload.wikimedia.org/wikipedia/commons/5/55/Euclidean_distance_2d.svg)\n",
        "\n",
        "While the concept is illustrated in 2D in the image above, it's scalable to n-dimensional vectors. The formula for Euclidean Distance in \\( n \\) dimensions is:\n",
        "\n",
        "$ (\\vec{u}, \\vec{v}) = \\| \\vec{u} - \\vec{v} \\| = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2} \\$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Given the vectors:\n",
        "$ \\ \\vec{u} = (2, 3, 4, 2) \\ $\n",
        "and\n",
        "$ \\ \\vec{v} = (1, -2, 1, 3) \\ $\n",
        "\n",
        "The Euclidean Distance between them is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "d(\\vec{u}, \\vec{v}) &= \\sqrt{(2-1)^2 + (3+2)^2 + (4-1)^2 + (2-3)^2} \\\\\n",
        "&= \\sqrt{1 + 25 + 9 + 1} \\\\\n",
        "&= \\sqrt{36} \\\\\n",
        "&= 6\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "AZOGVwv6maC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the NMF reduction\n",
        "\n",
        "\n",
        "print(data_reduced_nmf[0,:])\n",
        "print(data_reduced_nmf[1,:])"
      ],
      "metadata": {
        "id": "KqlF7jfVqMDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with numpy\n",
        "np.linalg.norm(data_reduced_nmf[0,:] - data_reduced_nmf[1,:])"
      ],
      "metadata": {
        "id": "1xLdr_P6qm59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "d0Lb7AJahkP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt((0.07242715-0.1211694)**2+(0.15113185-0.0946327)**2+(0-0.01483596)**2+(0.28106393-0.40376001)**2)"
      ],
      "metadata": {
        "id": "kWYB6se2hmBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(data_reduced_nmf[0,:] - data_reduced_nmf[2,:])"
      ],
      "metadata": {
        "id": "Tp9D4JuYqtSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(data_reduced_nmf[1,:] - data_reduced_nmf[2,:])"
      ],
      "metadata": {
        "id": "R21aqlad4Dch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['place'][:3]"
      ],
      "metadata": {
        "id": "EwE7yjtlqzP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or easier\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "metadata": {
        "id": "dEaP2Bqsr6AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "euclidean_matrix = euclidean_distances(data_reduced_nmf)\n",
        "euclidean_matrix.shape"
      ],
      "metadata": {
        "id": "OILXyctbsHzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argsort(euclidean_matrix[0,:])[:3]"
      ],
      "metadata": {
        "id": "uYJc97gwsN9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['place']=='Aalborg']"
      ],
      "metadata": {
        "id": "qaVbJT1VsgIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ixs = np.argsort(euclidean_matrix[588,:])[:10]\n",
        "print(data['place'][ixs])"
      ],
      "metadata": {
        "id": "jQmhdX1RsZjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommender_city(place, n_recs):\n",
        "  if place in list(set(data.place)):\n",
        "    ix = data[data['place']==place].index[0]\n",
        "    ixs = np.argsort(euclidean_matrix[ix,:])[n_recs:]\n",
        "    return data['place'][ixs]\n",
        "  else:\n",
        "    return 'Place not in the dataset'"
      ],
      "metadata": {
        "id": "pQ7FQMlus1l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommender_city('Beijing', 10)"
      ],
      "metadata": {
        "id": "TD4irXmUkaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "LBRZjXYv5BbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "CgThGE8j4_Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn=recommender_city,\n",
        "                    inputs= [gr.Dropdown(\n",
        "            data['place'].tolist(), label=\"City I like!\", info=\"Pick one!\"),\n",
        "\n",
        "                            gr.Slider(1, 15, 5, step=1,\n",
        "                            label=\"Number of recommendations\")],\n",
        "                    outputs=\"text\")"
      ],
      "metadata": {
        "id": "c0Rm4Uha49up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "G-YScL6s5q0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}